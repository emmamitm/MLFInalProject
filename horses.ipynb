{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races_df = pd.read_csv('races.csv', index_col='race_id')\n",
    "runs_df = pd.read_csv('runs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up dataframe\n",
    "\n",
    "create new dataframe df that we will use to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnessussary columns from runs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = runs_df\n",
    "df=df.drop('won',axis=1)\n",
    "df=df.drop('horse_gear',axis=1)\n",
    "df=df.drop('draw',axis=1)\n",
    "\n",
    "# the below have too many na values\n",
    "df = df.drop('horse_no', axis = 1)\n",
    "df=df.drop('position_sec1',axis=1)\n",
    "df=df.drop('position_sec2',axis=1)\n",
    "df=df.drop('position_sec3',axis=1)\n",
    "df=df.drop('position_sec4',axis=1)\n",
    "df=df.drop('position_sec5',axis=1)\n",
    "df=df.drop('position_sec6',axis=1)\n",
    "df=df.drop('behind_sec1',axis=1)\n",
    "df=df.drop('behind_sec2',axis=1)\n",
    "df=df.drop('behind_sec3',axis=1)\n",
    "df=df.drop('behind_sec4',axis=1)\n",
    "df=df.drop('behind_sec5',axis=1)\n",
    "df=df.drop('behind_sec6',axis=1)\n",
    "df=df.drop('time4',axis=1)\n",
    "df=df.drop('time5',axis=1)\n",
    "df=df.drop('time6',axis=1)\n",
    "df=df.drop('place_odds',axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add relevant columns from races_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, races_df[['venue','config','surface','distance','going']], on='race_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new columns to add to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### time metric columns\n",
    "these columns use the time1 time2 and time3 fields to determine how much of the final time was used to run different parts of the race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"time1\"] = df[\"time1\"] / df[\"finish_time\"]\n",
    "df[\"time2\"] = df[\"time2\"] / df[\"finish_time\"]\n",
    "df[\"time3\"] = df[\"time3\"] / df[\"finish_time\"]\n",
    "df[\"time23\"] = (df[\"time2\"] + df[\"time3\"]) / df[\"finish_time\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add horse, jockey, and trainer placement percentage columns\n",
    "\n",
    "add columns capturing the likelyhood of a given, horse, jockey, and trainer placing in the race"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New Horse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horse_tot_race=runs_df.groupby(['horse_id'])['result'].apply(lambda x: (x).sum()).reset_index(name='horse_tot_race')\n",
    "\n",
    "df=pd.merge(df,horse_tot_race,on='horse_id',how='left')\n",
    "\n",
    "horse_tot_place=runs_df.groupby(['horse_id'])['result'].apply(lambda x: (x <=3).sum()).reset_index(name='horse_tot_place')\n",
    "\n",
    "df=pd.merge(df,horse_tot_place,on='horse_id',how='left')\n",
    "\n",
    "df['horse_place_perc']=df['horse_tot_place']/df['horse_tot_race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New Jockey columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jockey_tot_race=runs_df.groupby(['jockey_id'])['result'].apply(lambda x: (x).sum()).reset_index(name='jockey_tot_race')\n",
    "\n",
    "df=pd.merge(df,jockey_tot_race,on='jockey_id',how='left')\n",
    "\n",
    "jockey_tot_place=runs_df.groupby(['jockey_id'])['result'].apply(lambda x: (x <=3).sum()).reset_index(name='jockey_tot_place')\n",
    "\n",
    "df=pd.merge(df,jockey_tot_place,on='jockey_id',how='left')\n",
    "\n",
    "df['jockey_place_perc']=df['jockey_tot_place']/df['jockey_tot_race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New Trainer Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_tot_race=runs_df.groupby(['trainer_id'])['result'].apply(lambda x: (x).sum()).reset_index(name='trainer_tot_race')\n",
    "\n",
    "df=pd.merge(df,trainer_tot_race,on='trainer_id',how='left')\n",
    "\n",
    "trainer_tot_place=runs_df.groupby(['trainer_id'])['result'].apply(lambda x: (x <=3).sum()).reset_index(name='trainer_tot_place')\n",
    "\n",
    "df=pd.merge(df,trainer_tot_place,on='trainer_id',how='left')\n",
    "\n",
    "df['trainer_place_perc']=df['trainer_tot_place']/df['trainer_tot_race']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove unneccessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('horse_tot_place',axis=1)\n",
    "df=df.drop('horse_tot_race',axis=1)\n",
    "df=df.drop('horse_id',axis=1)\n",
    "\n",
    "\n",
    "df=df.drop('trainer_tot_place',axis=1)\n",
    "df=df.drop('trainer_tot_race',axis=1)\n",
    "df=df.drop('trainer_id',axis=1)\n",
    "\n",
    "\n",
    "df=df.drop('jockey_tot_place',axis=1)\n",
    "df=df.drop('jockey_tot_race',axis=1)\n",
    "df=df.drop('jockey_id',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Anomalies function\n",
    "def iqr_anomalies(data, col):\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return data[(data[col] < lower_bound) | (data[col] > upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of rows before cleaning:\", df.shape[0])\n",
    "\n",
    "# drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "# handle anomalies\n",
    "race_ids = df['race_id'].unique()\n",
    "anomalie_race_ids = df.iloc[iqr_anomalies(df, \"finish_time\").index][\"race_id\"].unique()\n",
    "anomalie_race_indicies = df[df[\"race_id\"].isin(anomalie_race_ids)].index\n",
    "df = df.drop(anomalie_race_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "config_encoder = preprocessing.OrdinalEncoder()\n",
    "df['config'] = config_encoder.fit_transform(df['config'].values.reshape(-1, 1))\n",
    "\n",
    "going_encoder = preprocessing.OrdinalEncoder()\n",
    "df['going'] = going_encoder.fit_transform(df['going'].values.reshape(-1, 1))\n",
    "\n",
    "venue_encoder = preprocessing.LabelEncoder()\n",
    "df['venue'] = venue_encoder.fit_transform(df['venue'])\n",
    "\n",
    "horse_country_encoder = preprocessing.LabelEncoder()\n",
    "df['horse_country'] = horse_country_encoder.fit_transform(df['horse_country'])\n",
    "\n",
    "horse_type_encoder = preprocessing.LabelEncoder()\n",
    "df['horse_type'] = horse_type_encoder.fit_transform(df['horse_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save current state of dataframe\n",
    "\n",
    "We will want to reuse this state of the dataframe later, so save it as its own variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df\n",
    "\n",
    "test_1 = data[data.race_id==1601]\n",
    "test_2 = data[data.race_id==1602]\n",
    "test_3 = data[data.race_id==1603]\n",
    "test_4 = data[data.race_id==1604]\n",
    "test_5 = data[data.race_id==1605]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Number of Races up to Race 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only use up to race 1600 for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.race_id <= 1600]\n",
    "\n",
    "print(\"number of rows after cleaning:\", df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now drop race_id and result because they are not needed for training\n",
    "\n",
    "we are trying to predict the horse's time to finish the race, we did not drop them earlier because they were needed in the test_x dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('race_id', axis=1)\n",
    "df = df.drop('result', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df['finish_time']\n",
    "X = df.drop('finish_time', axis=1)\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,5))\n",
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegression = LinearRegression()\n",
    "linearRegression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linearRegression.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model - Linear Regression\n",
    "print(f\"Linear Regression Results:\")\n",
    "print(f\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(f\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(f\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnRegression = KNeighborsRegressor(n_neighbors=5)\n",
    "knnRegression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predKNN = knnRegression.predict(x_test)\n",
    "y_predKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model - KNN\n",
    "print(f\"KNN Regression Results:\")\n",
    "print(f\"Mean Squared Error:\", mean_squared_error(y_test, y_predKNN))\n",
    "print(f\"Mean Absolute Error:\", mean_absolute_error(y_test, y_predKNN))\n",
    "print(f\"R2 Score:\", r2_score(y_test, y_predKNN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeRegression = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "treeRegression.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model - Decision Tree\n",
    "y_predTree = treeRegression.predict(x_test)\n",
    "y_predTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Mean Squared Error:\", mean_squared_error(y_test, y_predTree))\n",
    "print(f\"Mean Absolute Error:\", mean_absolute_error(y_test, y_predTree))\n",
    "print(f\"R2 Score:\", r2_score(y_test, y_predTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regressor\n",
    "parameterGridTree = {\"max_depth\": [3,5,10, None], 'min_samples_split': [2,5,10]}\n",
    "gridTree = GridSearchCV(DecisionTreeRegressor(random_state=42), parameterGridTree, cv=5, scoring='neg_mean_squared_error')\n",
    "gridTree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters for Decision Tree:\", gridTree.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Regressor\n",
    "parameterGridKNN = {'n_neighbors': [3,5,7,10]}\n",
    "gridKNN = GridSearchCV(KNeighborsRegressor(), parameterGridKNN, cv=5, scoring='neg_mean_squared_error')\n",
    "gridKNN.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best Parameters for KNN:\", gridKNN.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators = 100, max_depth = 10)\n",
    "\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rfr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MSE: {mse}\")\n",
    "    print(f\"RMSE: {rmse}\")\n",
    "    print(f\"R² Score: {r2}\")\n",
    "\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "br = BaggingRegressor(estimator=None, n_estimators = 50, max_samples = .8,bootstrap = True)\n",
    "\n",
    "br.fit(x_train, y_train)\n",
    "\n",
    "y_pred = br.predict(x_test)\n",
    "\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AbaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "ada = AdaBoostRegressor(estimator=None, n_estimators = 100, learning_rate = 1)\n",
    "\n",
    "ada.fit(x_train, y_train)\n",
    "\n",
    "y_pred = ada.predict(x_test)\n",
    "\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "st = StackingRegressor(estimators=[('decision_tree', DecisionTreeRegressor()),('knn', KNeighborsRegressor())])\n",
    "\n",
    "st.fit(x_train, y_train)\n",
    "\n",
    "y_pred = st.predict(x_test)\n",
    "\n",
    "metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR\n",
    "\n",
    "Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "C = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_rbf = SVR(kernel=\"rbf\", C=C, epsilon=0.01).fit(x_train, y_train)\n",
    "y_pred_rbf = svr_rbf.predict(x_test)\n",
    "print(\"SVR RBF Results:\")\n",
    "print(f\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_rbf))\n",
    "print(f\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred_rbf))\n",
    "print(f\"R2 Score:\", r2_score(y_test, y_pred_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_poly = SVR(kernel=\"poly\", C=C, gamma=\"scale\").fit(x_train, y_train)\n",
    "y_pred_poly = svr_poly.predict(x_test)\n",
    "print(\"SVR Polynomial Results:\")\n",
    "print(f\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_poly))\n",
    "print(f\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred_poly))\n",
    "print(f\"R2 Score:\", r2_score(y_test, y_pred_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the deep residual network\n",
    "def deepResidualNetwork(input_shape):\n",
    "    model = Sequential([\n",
    "        #decided to use elu = exponential linear unit for activation function\n",
    "        #first layer--> has 256 neurons\n",
    "        Dense(256, activation='elu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        #dropout was picked to be moderate--> this is based on studies of NNs\n",
    "        Dropout(0.3),\n",
    "        #second layer--> has 128 neurons\n",
    "        Dense(128, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        #thrid layer--> has 64 neurons\n",
    "        Dense(64, activation='elu'),\n",
    "        BatchNormalization(),\n",
    "        #fourth layer--> has 32 neurons\n",
    "        Dense(32, activation='elu'),\n",
    "        #output layer--> 1 neuron for finish time\n",
    "        Dense(1)\n",
    "    ])\n",
    "    #using adam optimizer and a small alpha, \n",
    "    #for loss we are using huber--> it is more prone to outlier adaptation than MSE\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='huber')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating the pyramid neural network \n",
    "def pyramidNeuralNetwork(input_shape):\n",
    "    model = Sequential([\n",
    "        #first layer--> base of pyramid with 512 neurons\n",
    "        Dense(512, activation='selu', input_shape=(input_shape,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        #second layer--> half the neurons = 256 neurons\n",
    "        Dense(256, activation='selu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        #third layer--> half neurons again \n",
    "        Dense(128, activation='selu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        #fourth layer --> half neurons\n",
    "        Dense(64, activation='selu'),\n",
    "        BatchNormalization(),\n",
    "        #fifth layer--> half nuerons\n",
    "        Dense(32, activation='selu'),\n",
    "        #output= pyramid top\n",
    "        Dense(1)\n",
    "    ])\n",
    "    #using adam optimizer with a 0.01 alpha and the huber loss\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='huber')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleNeuralNetwork(input_shape):\n",
    "    model = Sequential([\n",
    "        # a single layer that directly maps inputs to output\n",
    "        Dense(1, input_shape=(input_shape,))\n",
    "    ])\n",
    "    #also using adam and huber\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='huber')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(): #monitoring val loss to earling stop or update learning rate \n",
    "    return [\n",
    "        #we are monitoring the val_loss\n",
    "        #val loss is a metric that calculates how well the model will perform on unseen data\n",
    "        EarlyStopping( #stop early if neccesary\n",
    "            monitor='val_loss',\n",
    "            patience=10, #waits 10 epochs for improvement\n",
    "            restore_best_weights=True #we want to keep the weaits that give the best val loss\n",
    "        ),\n",
    "        ReduceLROnPlateau( #we will reduce learning rate when we see it plateau on performance\n",
    "            monitor='val_loss', #monitoring val loss to see if LR needs adjustment \n",
    "            factor=0.5, #multiplies learning rate if need be\n",
    "            patience=10, #waits 10 epochs\n",
    "            min_lr=0.0001 #set a lower bound \n",
    "        )\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and evaluating the model\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test, model_name): \n",
    "    callbacks = get_callbacks()\n",
    "    history = model.fit(\n",
    "        X_train, y_train, #splitting x training and y training\n",
    "        validation_split=0.2, #takes 20 percent of trainng data for validation\n",
    "        epochs=100, #using 100 epochs \n",
    "        batch_size=32, #number of samples that we process before model updates\n",
    "        callbacks=callbacks,#may want to correct model during training\n",
    "        verbose=1 #print what is happening\n",
    "    )\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred) #calculates MSE between predicted and actual\n",
    "    r2 = r2_score(y_test, y_pred)# calculates coeff of determination\n",
    "    return history, mse, r2, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create testing samples to test the Neural Networks \n",
    "test_races = pd.concat([test_1, test_2, test_3, test_4, test_5]) #all races we are testing \n",
    "X_test_nn = test_races.drop(['race_id', 'result', 'finish_time'], axis=1) #remove columns not needed \n",
    "X_test_nn = pd.DataFrame(scaler.fit_transform(X_test_nn), columns=X_test_nn.columns) #x test set\n",
    "y_test_nn = test_races['finish_time'] #y test set\n",
    "\n",
    "#all models that we run on test data\n",
    "models = {\n",
    "    'Simple NN': simpleNeuralNetwork(X.shape[1]), #Regular NN\n",
    "    'Deep Residual NN': deepResidualNetwork(X.shape[1]), #deep residual NN\n",
    "    'Pyramid NN': pyramidNeuralNetwork(X.shape[1]), #pyramid NN\n",
    "}\n",
    "results = {} #store training results \n",
    "predictions = {} #store model predictions\n",
    "\n",
    "# train and evaluate each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    history, mse, r2, y_pred = evaluate_model(\n",
    "        model, \n",
    "        x_train, \n",
    "        y_train, \n",
    "        X_test_nn, \n",
    "        y_test_nn, \n",
    "        name\n",
    "    )\n",
    "    results[name] = {\n",
    "        'history': history,\n",
    "        'mse': mse,\n",
    "        'r2': r2\n",
    "    }\n",
    "    predictions[name] = y_pred.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all visualization functions\n",
    "#learning curve to watch alpha change \n",
    "def learningCurve():\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for name, result in results.items():\n",
    "        plt.plot(result['history'].history['loss'], label=f'{name} - Training')\n",
    "        plt.plot(result['history'].history['val_loss'], label=f'{name} - Validation')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Huber Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "#a scatter plot to see how well each model performs \n",
    "def modelPredictions():\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.scatter(range(len(y_test_nn)), y_test_nn, label='Actual', alpha=0.5)\n",
    "    for name, pred in predictions.items():\n",
    "        plt.scatter(range(len(pred)), pred, label=f'{name} Predicted', alpha=0.5)\n",
    "    plt.title('Actual vs Predicted Finish Times')\n",
    "    plt.xlabel('Race Instance')\n",
    "    plt.ylabel('Finish Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "#print results of the models \n",
    "def performanceMetrics():\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, result in results.items():\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"Mean Squared Error: {result['mse']:.4f}\")\n",
    "        print(f\"Root Mean Squared Error: {np.sqrt(result['mse']):.4f}\")\n",
    "        print(f\"R² Score: {result['r2']:.4f}\")\n",
    "learningCurve()\n",
    "modelPredictions()\n",
    "performanceMetrics()\n",
    "\n",
    "#show best model\n",
    "best_model_name = min(results, key=lambda x: results[x]['mse'])\n",
    "best_model = models[best_model_name]\n",
    "print(f\"\\nBest performing model was {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Models Against New Races\n",
    "\n",
    "Now that we have trained multiple models, we will compare them by testing them against the 5 test races we extracted earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_races = [data[data.race_id == race_id].copy() for race_id in [1601, 1602, 1603, 1604, 1605]]\n",
    "\n",
    "knn_preds = []\n",
    "\n",
    "tree_preds = []\n",
    "\n",
    "svr_preds = []\n",
    "\n",
    "nn_preds = []\n",
    "\n",
    "for race in test_races:\n",
    "    X = race.drop([\"finish_time\", \"result\", \"race_id\"], axis=1)\n",
    "    X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "\n",
    "    knn_preds.append(knnRegression.predict(X))\n",
    "\n",
    "    tree_preds.append(treeRegression.predict(X))\n",
    "\n",
    "    svr_preds.append(svr_rbf.predict(X))\n",
    "\n",
    "    # add new columns with nn predictions here\n",
    "    # nn_preds.append(nn.predict(race)) \n",
    "\n",
    "\n",
    "for i, race in enumerate(test_races):\n",
    "    race[\"knn_pred\"] = knn_preds[i]\n",
    "    race[\"knn_result\"] = race[\"knn_pred\"].rank(ascending=True).astype(int)\n",
    "\n",
    "    race[\"tree_pred\"] = tree_preds[i]\n",
    "    race[\"tree_result\"] = race[\"tree_pred\"].rank(ascending=True).astype(int)\n",
    "\n",
    "    race[\"svr_pred\"] = svr_preds[i]\n",
    "    race[\"svr_result\"] = race[\"svr_pred\"].rank(ascending=True).astype(int)\n",
    "\n",
    "    # add new columns with nn predictions here\n",
    "    # test_races[i][\"nn_pred\"] = nn_preds[i]\n",
    "    # test_races[i][\"nn_result\"] = test_races[i][\"nn_result\"].rank(ascending=True).astype(int)\n",
    "\n",
    "    # make sure to add your columns to this list\n",
    "    test_races[i] = race[['finish_time', 'result', \n",
    "                          'knn_pred', 'knn_result',\n",
    "                          'tree_pred', 'tree_result',\n",
    "                          'svr_pred', 'svr_result']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_races[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_races[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_races[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_races[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_races[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columns\n",
    "\n",
    "finish_time\t\n",
    "\n",
    "actual_weight\t\n",
    "\n",
    "config\t\n",
    "\n",
    "declared_weight\t\n",
    "\n",
    "distance\t\n",
    "\n",
    "going\t\n",
    "\n",
    "horse_age\t\n",
    "\n",
    "horse_country\t\n",
    "\n",
    "horse_place_perc\t\n",
    "\n",
    "horse_type\t\n",
    "\n",
    "jockey_place_perc\t\n",
    "\n",
    "time1/finising_time\n",
    "\n",
    "time2/finising_time\n",
    "\n",
    "time3/finising_time\n",
    "\n",
    "surface\t\n",
    "\n",
    "trainer_place_perc\t\n",
    "\n",
    "venue\n",
    "\n",
    "quartile = result / # horses ....\n",
    "\n",
    "_______\n",
    "\n",
    "result & race_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exploratory data visualization, want both uni and multivariate visualizations for analysis\n",
    "\n",
    "more data preprocessing and cleanning\n",
    "\n",
    "experiment with feature selection techniques\n",
    "\n",
    "show variety of algorithms, log reg, knn, decision trees, rand forrests, ensemble techniques, SVMs (all doable with scikit learn) build NN and DNN to expieriment with different architecture of NNs, maybe take a representative sample of 20000 or so examples (carful not to lose too much info)\n",
    "\n",
    "meet as team ASAP and make final plan \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
